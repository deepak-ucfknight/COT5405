{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_Entropy_loss_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepak-ucfknight/COT5405/blob/master/Binary_Entropy_loss_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Y0gdluhk5W7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import headers and packages\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7i7byV9C5dC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variables\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 1\n",
        "epochs = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NzN4d3U65lsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Image Dimensions of MNIST\n",
        "\n",
        "rows = 28\n",
        "cols = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PODHPe2L5r5f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loading MNIST dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, rows, cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, rows, cols)\n",
        "    input_shape = (1, rows, cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
        "    input_shape = (rows, cols, 1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PoRj1qtq50_5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Normalization\n",
        "\n",
        "x_train = x_train / 255;\n",
        "x_test = x_test / 255;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzXBR2pmJwz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape data\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLQyBFqn6O8Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Helper Functions***"
      ]
    },
    {
      "metadata": {
        "id": "ybeU8Y-o6Vko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function intitalize weights and bias to zero\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "   w = np.zeros(shape=(dim, num_classes))\n",
        "   b = 0\n",
        "   return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DryivKa86auF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "   return 1.0/(1.0+np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_iBwvaRG6jVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute sigmoid prime\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vO2n8W8Y6qvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute Minibatches.\n",
        "\n",
        "def mini_batches(X, Y, batchsize):\n",
        "    for start_idx in range(0, X.shape[0] - batchsize + 1, batchsize):\n",
        "        excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield X[excerpt], Y[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L53eeSe27D1D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute binary entropy loss\n",
        "\n",
        "def binary_entropy(w,b,X,Y):\n",
        "  \n",
        "  m = X.shape[1]\n",
        "  A = sigmoid(np.dot(w.T, X) + b)\n",
        "  cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) # binary entropy\n",
        " \n",
        "  # binary entropy gradients computation\n",
        "  dw = (1 / m) * np.dot(X, (A - Y).T)\n",
        "  db = (1 / m) * np.sum(A - Y)\n",
        "  \n",
        "  grads = {\"dw\": dw,\n",
        "            \"db\": db }\n",
        "  \n",
        "  cost = np.squeeze(cost)\n",
        "    \n",
        "  return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7w2Q3hrR7URW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for back propogation of gradients\n",
        "def optimize(w, b, X, Y, epochs, learning_rate, print_cost = False):\n",
        "  \n",
        "  costs = []\n",
        "    \n",
        "  for i in range(epochs):\n",
        " \n",
        "    for batch in mini_batches(X.T, Y.T, batch_size):\n",
        " \n",
        "       x_batch, y_batch = batch\n",
        "       grads, cost = binary_entropy(w, b, x_batch.T, y_batch.T)\n",
        "       \n",
        "       # gradient descent\n",
        "       dw = grads[\"dw\"]\n",
        "       db = grads[\"db\"]\n",
        "       w = w - learning_rate * dw \n",
        "       b = b - learning_rate * db\n",
        "\n",
        "       \n",
        "    costs.append(cost)\n",
        "\n",
        "    if print_cost:\n",
        "       print (\"Loss after iteration %i: %f\" % (i, cost))\n",
        "            \n",
        "  params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "\n",
        "  grads = {\"dw\": dw,\n",
        "               \"db\": db}\n",
        "\n",
        "  return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXEe1eeS7do6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for predicting the values\n",
        "\n",
        "def predict(w, b, X):\n",
        "   \n",
        "   m = X.shape[1] #number of samples\n",
        "   Y_prediction = np.zeros((1, m)) # shape of 1,60000 - contains whether 0 or 1 depending upong the confidence value\n",
        "   w = w.reshape(X.shape[0], num_classes)\n",
        "   A = sigmoid(np.dot(w.T, X) + b) # shape of 1,60000 - contains values from 0 to 1 denoting the confidence value\n",
        "  \n",
        "   for i in range(A.shape[1]):\n",
        "      Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0  # storing 0 or 1 based on the confidence value of the predicted output\n",
        "  \n",
        "   return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RvNj2ZkB7kZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=50, learning_rate=0.1, print_cost=False):\n",
        "  \n",
        "\n",
        "  predict_Y_train = Y_train;\n",
        "  predict_Y_test = Y_test;\n",
        "  \n",
        "  # shape of predict_y_train and predict_y_test is 60000,10 similar to that of y_train and y_test categorical values\n",
        "  predict_Y_train = keras.utils.to_categorical(predict_Y_train, 10)\n",
        "  predict_Y_test = keras.utils.to_categorical(predict_Y_test, 10)\n",
        "  \n",
        "  \n",
        "  models = { }  #dict to hold the trained models.\n",
        "  \n",
        "  for i in range(0,10):\n",
        "      # set the digit to classify\n",
        "      classifier_digit = i;\n",
        "      \n",
        "      # modify training labels to create single class classification\n",
        "      y_train_mod = np.array(Y_train);\n",
        "      y_train_mod = np.where(y_train_mod == classifier_digit, 1, 0)\n",
        "      \n",
        "      y_test_mod = np.array(Y_test);\n",
        "      y_test_mod = np.where(y_test_mod == classifier_digit, 1, 0)\n",
        "      \n",
        "      # core of the model\n",
        "      w, b = initialize_with_zeros(X_train.shape[0])\n",
        "      parameters, grads, costs = optimize(w, b, X_train, y_train_mod, num_iterations, learning_rate, print_cost)\n",
        "      w = parameters[\"w\"]\n",
        "      b = parameters[\"b\"]\n",
        "\n",
        "      # predicting the values based on trained weights and bias values\n",
        "      Y_prediction_train = predict(w, b, X_train)\n",
        "      Y_prediction_test = predict(w,b, X_test) \n",
        "      \n",
        "      \n",
        "      # we will store ones in the corresponding to indices to create array to similar to categorical data of y_train\n",
        "      predict_Y_train[:,[i]] = Y_prediction_train.T\n",
        "      predict_Y_test[:,[i]] = Y_prediction_test.T\n",
        "      \n",
        "       # train and test classifier accuracy for each digit\n",
        "      print(\"train accuracy of classifer \" + str(i) +\" : {} %\".format(100 - np.mean(np.abs(Y_prediction_train - y_train_mod)) * 100))\n",
        "      print(\"test accuracy of classifer \" + str(i) +\" : {} %\".format(100 - np.mean(np.abs(Y_prediction_test - y_test_mod)) * 100))\n",
        "      \n",
        "\n",
        "      d = { \"costs\": costs,\n",
        "            \"Y_prediction_test\": Y_prediction_test, \n",
        "            \"Y_prediction_train\" : Y_prediction_train, \n",
        "            \"w\" : w, \n",
        "            \"b\" : b,\n",
        "            \"learning_rate\" : learning_rate,\n",
        "            \"num_iterations\": num_iterations }\n",
        "\n",
        "      models[i] = d\n",
        "    \n",
        "    \n",
        "  # coverting y_train from 60000,1 to 60000,10 i.e. to categorical \n",
        "  Y_train = keras.utils.to_categorical(Y_train, 10)\n",
        "  Y_test = keras.utils.to_categorical(Y_test, 10)\n",
        "  \n",
        "    \n",
        "  # overall test and train classifier accuracy of the network\n",
        "  print(\"Overall train accuracy: {} %\".format(100 - np.mean(np.abs(predict_Y_train - Y_train)) * 100))\n",
        "  print(\"Overall test accuracy: {} %\".format(100 - np.mean(np.abs(predict_Y_test - Y_test)) * 100))\n",
        "\n",
        "  return models;\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvODfWNr9ebg",
        "colab_type": "code",
        "outputId": "eb35a41c-611b-44bb-f849-2409409437de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "classifier = model(x_train, y_train, x_test, y_test, num_iterations = 12, learning_rate = 0.1, print_cost = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy of classifer 0 : 99.26333333333334 %\n",
            "test accuracy of classifer 0 : 99.21 %\n",
            "train accuracy of classifer 1 : 99.11833333333334 %\n",
            "test accuracy of classifer 1 : 99.3 %\n",
            "train accuracy of classifer 2 : 97.735 %\n",
            "test accuracy of classifer 2 : 97.52 %\n",
            "train accuracy of classifer 3 : 97.62166666666667 %\n",
            "test accuracy of classifer 3 : 97.86 %\n",
            "train accuracy of classifer 4 : 98.41666666666667 %\n",
            "test accuracy of classifer 4 : 98.34 %\n",
            "train accuracy of classifer 5 : 97.41333333333333 %\n",
            "test accuracy of classifer 5 : 97.6 %\n",
            "train accuracy of classifer 6 : 98.79 %\n",
            "test accuracy of classifer 6 : 98.78 %\n",
            "train accuracy of classifer 7 : 98.42166666666667 %\n",
            "test accuracy of classifer 7 : 98.44 %\n",
            "train accuracy of classifer 8 : 95.915 %\n",
            "test accuracy of classifer 8 : 95.96 %\n",
            "train accuracy of classifer 9 : 96.34833333333333 %\n",
            "test accuracy of classifer 9 : 96.41 %\n",
            "Overall train accuracy: 97.90433328598738 %\n",
            "Overall test accuracy: 97.94200006872416 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}