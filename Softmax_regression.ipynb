{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepak-ucfknight/COT5405/blob/master/Softmax_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sIxtYDgFdSnN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import headers and packages\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VopAVn4jdWbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#variables\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oBnyxsFdZre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Image Dimensions of MNIST\n",
        "rows = 28\n",
        "cols = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FjImg7zidce6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loading MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, rows, cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, rows, cols)\n",
        "    input_shape = (1, rows, cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
        "    input_shape = (rows, cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SwRsb2dXdh1W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Normalization\n",
        "\n",
        "x_train = x_train / 255;\n",
        "x_test = x_test / 255;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dVp_Y0eudka2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " y_train = keras.utils.to_categorical(y_train, 10)\n",
        " y_test = keras.utils.to_categorical(y_test, 10)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSYfnwUbd3D8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape data\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hVJkROGd7nB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Helper Functions***"
      ]
    },
    {
      "metadata": {
        "id": "CZQdKDJ7eAcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function intitalize weights and bias to zero\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "   w = np.zeros(shape=(dim, num_classes))\n",
        "   b = 0\n",
        "   return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "duyKwM0PeDm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute Minibatches.\n",
        "def mini_batches(X, Y, batchsize):\n",
        "    for start_idx in range(0, X.shape[0] - batchsize + 1, batchsize):\n",
        "        excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield X[excerpt], Y[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-2DNL8ieHv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  z -= np.max(z)\n",
        "  return np.exp(z) / np.sum(np.exp(z), axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2O2ZycN8lZR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax_grad(z):\n",
        "  s = z.reshape(-1,1)\n",
        "  return np.diagflat(s) - np.dot(s, s.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APkSWje7fXdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy(w,b,X,Y):\n",
        " \n",
        "  m = X.shape[1]\n",
        "  A = softmax(np.dot(w.T, X) + b)\n",
        "  \n",
        "  cost = -np.mean(Y * np.log(A))\n",
        "  \n",
        "  \n",
        "  \n",
        "  dw = (-1 / m) * np.dot(X, (Y - A).T)\n",
        "  db = (-1 / m) * np.sum(Y-A)\n",
        "  \n",
        "  grads = {\"dw\": dw,\n",
        "            \"db\": db }\n",
        "  \n",
        "  cost = np.squeeze(cost)\n",
        "    \n",
        "  return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQdDkqQMiC6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for back propogation of gradients\n",
        "def optimize(w, b, X, Y, epochs, learning_rate, print_cost = False):\n",
        "  \n",
        "  costs = []\n",
        "    \n",
        "  for i in range(epochs):\n",
        " \n",
        "    for batch in mini_batches(X.T, Y.T, batch_size):\n",
        " \n",
        "       x_batch, y_batch = batch\n",
        "       grads, cost = cross_entropy(w, b, x_batch.T, y_batch.T)\n",
        "       dw = grads[\"dw\"]\n",
        "       db = grads[\"db\"]\n",
        "       w = w - learning_rate * dw \n",
        "       b = b - learning_rate * db\n",
        "\n",
        "       \n",
        "    costs.append(cost)\n",
        "\n",
        "    if print_cost:\n",
        "       print (\"Loss after iteration %i: %f\" % (i, cost))\n",
        "            \n",
        "  params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "\n",
        "  grads = {\"dw\": dw,\n",
        "               \"db\": db}\n",
        "\n",
        "  return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCb3s7AsiP5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for predicting the values\n",
        "def predict(w, b, X):\n",
        "  m = X.shape[1]\n",
        "  Y_prediction = np.zeros((num_classes, m))\n",
        "  w = w.reshape(X.shape[0], num_classes)\n",
        "  A = softmax(np.dot(w.T, X) + b)\n",
        "  \n",
        "  Y_prediction[np.arange(len(A)), A.argmax(1)] = 1\n",
        "  \n",
        "  return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PXORnZqOiRC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=50, learning_rate=0.1, print_cost=False):\n",
        "  \n",
        "  w, b = initialize_with_zeros(X_train.shape[0])\n",
        "  parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "  w = parameters[\"w\"]\n",
        "  b = parameters[\"b\"]\n",
        "\n",
        "\n",
        "  Y_prediction_train = predict(w, b, X_train)\n",
        "  Y_prediction_test = predict(w,b, X_test)\n",
        "  \n",
        "  \n",
        "  print(\"train accuracy : {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "  print(\"test accuracy  : {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "      \n",
        "\n",
        "  d = { \"costs\": costs,\n",
        "        \"Y_prediction_test\": Y_prediction_test, \n",
        "        \"Y_prediction_train\" : Y_prediction_train, \n",
        "        \"w\" : w, \n",
        "        \"b\" : b,\n",
        "        \"learning_rate\" : learning_rate,\n",
        "        \"num_iterations\": num_iterations }\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xV85ysZ8i-0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7a9caa2c-665a-455b-d95a-826306bdfeb3"
      },
      "cell_type": "code",
      "source": [
        "classifier = model(x_train, y_train, x_test, y_test, num_iterations = 12, learning_rate = 0.01, print_cost = True)"
      ],
      "execution_count": 534,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 0.035348\n",
            "Loss after iteration 1: 0.024838\n",
            "Loss after iteration 2: 0.020828\n",
            "Loss after iteration 3: 0.018670\n",
            "Loss after iteration 4: 0.017292\n",
            "Loss after iteration 5: 0.016317\n",
            "Loss after iteration 6: 0.015578\n",
            "Loss after iteration 7: 0.014990\n",
            "Loss after iteration 8: 0.014506\n",
            "Loss after iteration 9: 0.014097\n",
            "Loss after iteration 10: 0.013745\n",
            "Loss after iteration 11: 0.013436\n",
            "train accuracy : 90.00166666666667 %\n",
            "test accuracy  : 90.01 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}